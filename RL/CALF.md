Окей, давай сделаем «паспорт проекта» по CALF и relax probability.

Я напишу так, чтобы ты мог просто кинуть этот файл в новый диалог — и агент сразу понимал, о чём речь.

---

# Проект: CALF и relax probability (по диссертации Osinenko 2024)

## 0. Что за проект и зачем

Мы изучаем подход **Critic as Lyapunov function (CALF)** из хабилитационной диссертации
Pavel Osinenko *“Reinforcement Learning with Guarantees”* (2024) и связанной статьи
**“Critic as Lyapunov function (CALF): a model-free, stability-ensuring agent”**.

Ключевая идея:
**критик** в RL используется **как функция Ляпунова**, а поверх обычного RL-актора строится механизм, который:

* гарантирует **достижение и удержание цели** $$G$$ (goal-reaching property), если есть номинальная безопасная политика $$\pi_0$$;
* при этом всё ещё позволяет агенту **обучаться и исследовать**;
* вводит параметр **relax probability** $$P_{\text{relax}}$$, который описывает, с какой вероятностью система разрешает себе «не включать» безопасную политику, даже если сертификат Ляпунова не получился.

В этом проекте нас интересует:

1. как формально устроен CALF;
2. как именно вводится и работает **relax probability**;
3. какие гарантии при этом получаются.

---

## 1. Базовая постановка

### 1.1. MDP и цель

Система моделируется как **марковский процесс принятия решений** (MDP):

* $$S$$ — пространство состояний;
* $$A$$ — пространство действий;
* $$p(\cdot \mid s,a)$$ — переходы;
* $$r(s,a)$$ или $$c(s,a)$$ — награда или стоимость.

Классическая RL-задача:

$$
v^\pi(s) = \mathbb{E}*{A_t \sim \pi(\cdot|S_t)} \left[\sum*{t=0}^{\infty} \gamma^t r(S_t, A_t) \mid S_0 = s\right],
$$
нужно подобрать политику $$\pi$$, максимизирующую/минимизирующую этот функционал.

В диссертации выделяется **целевая область** $$G \subset S$$, обычно компактная окрестность нуля:

$$
d_G(s) := \inf_{s' \in G} |s - s'|,
$$
— расстояние до цели.

### 1.2. Номинальная безопасная политика $$\pi_0$$

Определяется класс политик $$\Pi_0$$, которые обладают **goal-reaching property**:

$$
\forall s_0 \in S:\quad
\mathbb{P}\big(d_G(S_t^{\pi_0}(s_0)) \xrightarrow[t\to\infty]{} 0\big) \ge 1 - \eta,\quad \eta \in [0,1).
$$

То есть под управлением $$\pi_0$$ система почти наверняка приходит в $$G$$ (и остаётся возле неё), с вероятностью неудачи не более $$\eta$$.

---

## 2. Идея CALF: критик как функция Ляпунова

### 2.1. Критик

Есть **критик** (action-value function) $$\hat q_w(s,a)$$ — аппроксиматор (обычно нейросеть) с параметрами $$w\in W$$.

Фишка CALF: $$-\hat q_w(\cdot,\cdot)$$ интерпретируется как **кандидат в функцию Ляпунова** относительно цели $$G$$:

* чем ближе к цели, тем ближе значение к 0;
* чем дальше, тем ниже (более отрицательное).

Вводятся функции класса $$\mathcal K_\infty$$:

$$
\hat\kappa_{\text{low}}(\cdot),\quad \hat\kappa_{\text{up}}(\cdot),
$$

и накладываются ограничения:

$$
\hat\kappa_{\text{low}}(|s|) \le -\hat q_w(s,a) \le \hat\kappa_{\text{up}}(|s|).
$$

Типичный выбор (пример из текста):

$$
\hat\kappa_{\text{low}}(r) = C_{\text{low}} r^2,\quad
\hat\kappa_{\text{up}}(r) = C_{\text{up}} r^2,\quad 0 < C_{\text{low}} < C_{\text{up}}.
$$

Это говорит: $$-\hat q_w$$ растёт примерно как $$|s|^2$$.

### 2.2. «Сертифицированная» тройка $$(s^\dagger, a^\dagger, w^\dagger)$$

Алгоритм хранит последнюю тройку, для которой критик был успешно обновлён:

$$
(s^\dagger, a^\dagger, w^\dagger).
$$

При каждом новом шаге $$t$$ он пытается найти новый набор параметров $$w^*$$, решая оптимизационную задачу:

$$
w^* = \arg\min_{w \in W} L_{\text{crit}}(w)
$$

при условиях:

1. **Ляпунов-подобное уменьшение:**

   $$
   \hat q_w(s_t, a_t) - \hat q_{w^\dagger}(s^\dagger, a^\dagger) \ge \bar\nu,
   $$

   где $$\bar\nu > 0$$ — фиксированный порог.

   Поскольку $$-\hat q$$ — «функция Ляпунова», это означает, что при переходе от $$(s^\dagger,a^\dagger)$$ к $$(s_t,a_t)$$ значение **увеличивает** $$\hat q$$, то есть $$-\hat q$$ **уменьшается минимум на** $$\bar\nu$$ → система движется к цели.

2. **K(_\infty)-ограничения:**

   $$
   \hat\kappa_{\text{low}}(|s_t|) \le -\hat q_w(s_t, a_t) \le \hat\kappa_{\text{up}}(|s_t|).
   $$

Если решение $$w^*$$ найдено — тройка $$(s^\dagger,a^\dagger,w^\dagger)$$ обновляется на $$(s_t,a_t,w^*)$$.

Если **решения нет**, это означает: **текущее действие актора нельзя сертифицировать как “хороший шаг по Ляпунову”**.

---

## 3. Алгоритм CALF (словами)

Алгоритм 1 в диссертации:

1. **Setup:** MDP, критик $$\hat q_w$$, актор $$\pi_t$$, номинальная политика $$\pi_0$$, функции $$\hat\kappa_{\text{low}}, \hat\kappa_{\text{up}}$$, порог $$\bar\nu$$, параметры relax:
   $$0 \le \lambda_{\text{relax}} < 1$$.

2. **Инициализация:**

   * начальное состояние $$s_0$$;
   * действие $$a_0 \sim \pi_0(\cdot \mid s_0)$$;
   * начальные веса $$w_0$$ критика, удовлетворяющие K(_\infty)-ограничениям;
   * $$(s^\dagger,a^\dagger,w^\dagger) \leftarrow (s_0,a_0,w_0)$$;
   * $$P_{\text{relax}} \leftarrow \lambda_{\text{relax}}$$.

3. Для $$t = 1,2,\dots$$:

   1. Выполнить предыдущее действие $$a_{t-1}$$ и наблюдать новое состояние $$s_t$$.
   2. **Обновить политику**:
      $$\pi_t = \arg\min_{\pi} L_{\text{act}}(\pi)$$
      (обычный RL-апдейт, можно с $$\varepsilon$$-жадным исследованием).
   3. Сэмплировать действие $$a_t \sim \pi_t(\cdot \mid s_t)$$.
   4. **Попробовать обновить критик**: решить задачу минимизации $$L_{\text{crit}}$$ при Ляпунов-ограничениях (см. выше).
   5. Если $$w^*$$ найден, обновить $$(s^\dagger,a^\dagger,w^\dagger)$$.
   6. Сэмплировать $$q \sim U[0,1]$$.
   7. Если **решение не найдено** и **$$q \ge P_{\text{relax}}$$**, то **принудительно переключиться на** $$\pi_0$$:
      $$
      \pi_t(\cdot \mid s_t) \leftarrow \pi_0(\cdot \mid s_t).
      $$
   8. Обновить **relax probability**:
      $$
      P_{\text{relax}} \leftarrow \lambda_{\text{relax}} P_{\text{relax}}.
      $$

Критически важно: CALF не «запрещает» какие-то состояния, как жёсткий shield. Если $$\pi_0$$ включился и вёл систему по безопасной траектории $$(s_t, a_t)$$, эти траектории **всё равно участвуют в обучении критика**, давая верхнюю оценку стоимости, которую потом можно улучшать.

---

## 4. Relax probability $$P_{\text{relax}}$$ и relaxation factor $$\lambda_{\text{relax}}$$

### 4.1. Определение

В алгоритме есть две величины:

* $$P_{\text{relax}} \in [0,1)$$ — **вероятность расслабиться**, то есть **оставить действие актора**, **не вызывая** $$\pi_0$$, даже если критик не нашёл допустимого $$w^*$$;
* $$\lambda_{\text{relax}} \in [0,1)$$ — **relaxation factor**, коэффициент геометрического убывания $$P_{\text{relax}}$$.

**Remark 2** в диссертации:

> Переменная $$P_{\text{relax}}$$ определяет вероятность пропустить вызов $$\pi_0$$, когда этот вызов иначе бы произошёл. На каждом шаге эта вероятность убывает с коэффициентом $$\lambda_{\text{relax}} < 1$$, называемым «relaxation factor». Чем больше $$\lambda_{\text{relax}}$$, тем больше свободы у агента во время обучения. Выбор $$\lambda_{\text{relax}}$$ не влияет на сам факт goal-reaching, но большие значения могут ухудшить худший случай по времени достижения цели.

Механика:

* Условие в алгоритме:
  если $$w^*$$ не найден и $$q \ge P_{\text{relax}}$$, то включаем $$\pi_0$$.
* При $$q \sim U[0,1]$$:

  $$
  \mathbb{P}(\text{вызвать }\pi_0) = \mathbb{P}(q \ge P_{\text{relax}}) = 1 - P_{\text{relax}},
  $$
  $$
  \mathbb{P}(\text{НЕ вызывать }\pi_0) = P_{\text{relax}}.
  $$

То есть **$$P_{\text{relax}}$$ — это именно вероятность "расслабить щит"** и позволить актору сделать несертифицированное действие.

Обновление:

$$
P_{\text{relax}}(t) = \lambda_{\text{relax}}^{,t+1},
$$

если стартовать с $$P_{\text{relax}} = \lambda_{\text{relax}}$$.

* В начале: $$P_{\text{relax}}$$ достаточно большой ⇒ щит часто выключен ⇒ много исследования.
* Со временем: $$P_{\text{relax}} \to 0$$ ⇒ почти всегда при проблемах с сертификатом зовём безопасную $$\pi_0$$.

### 4.2. Индикатор «расслабления» и конечное число таких шагов

В доказательстве теоремы вводится индикатор:

$$
\xi_t = \mathbf{1}{q < P_{\text{relax}}},\quad \xi_0 = 1.
$$

Здесь $$\xi_t = 1$$ означает: «на шаге $$t$$ произошёл **relax** — **не вызвали** $$\pi_0$$ там, где её могли бы вызвать».

Показывается, что:

$$
\mathbb{E}\Big[ \sum_{i=0}^\infty \xi_i \Big]
\le \frac{1}{1 - \lambda_{\text{relax}}}.
$$

Дальше применяют неравенство Маркова:

$$
\forall C > 0:\quad
\mathbb{P}\Big( \sum_{i=0}^\infty \xi_i \ge C \Big)
\le \frac{1}{C(1 - \lambda_{\text{relax}})},
$$

и отсюда вывод:

$$
\mathbb{P}\Big( \sum_{i=0}^\infty \xi_i = \infty \Big) = 0.
$$

Интерпретация:

* ожидаемое количество шагов, когда щит «расслабляется», **конечно**;
* с вероятностью 1 таких шагов **будет только конечное число**;
* после какого-то случайного, но конечного времени агент **перестаёт отключать** $$\pi_0$$ в ситуациях без сертификата.

Это ключевой элемент для сохранения гарантий.

---

## 5. Гарантии CALF (с учётом relax probability)

Есть теорема (в диссертации как Theorem 1 / Theorem 2 в приложении):

> Если номинальная политика $$\pi_0$$ обладает goal-reaching property для множества $$G$$ (как выше),
> и политика $$\pi_t$$ порождается алгоритмом CALF,
> то $$\pi_t$$ **сохраняет тот же goal-reaching property**:
>
> $$
> \forall s_0 \in S:\quad \mathbb{P}\big(d_G(S_t^{\pi_t}(s_0)) \to 0\big) \ge 1 - \eta.
> $$

Более сильное утверждение (при некотором выборе правил обнуления $$P_{\text{relax}},\lambda_{\text{relax}}$$ в малой окрестности цели) даёт ещё и **равномерную оценку overshoot'а**:

* для любого $$\varepsilon > 0$$ можно выбрать такую малую окрестность $$d_G(s) < \delta$$, что если стартовать в ней, то с вероятностью $$\ge 1-\eta$$:

  * расстояние до цели $$d_G(S_t)$$ стремится к нулю;
  * и при этом $$\sup_t d_G(S_t) \le \varepsilon$$.

Интуитивно: CALF не только ведёт систему к цели, но и гарантирует, что **при достаточно малых начальных состояниях траектория не вылезет далеко наружу** (overshoot ограничен).

При этом:

* значение $$\lambda_{\text{relax}}$$ **не влияет на сам факт goal-reaching**;
* но зависит **оценка времени достижения** (большие $$\lambda_{\text{relax}}$$ → дольше исследование → худший случайнее hitting time).

---

## 6. Интуитивный «elevator pitch» для будущей версии ассистента

Если в новом диалоге ты скажешь: «напомни, что за проект с CALF и relax probability», то ассистент должен примерно так объяснять:

> Мы рассматриваем подход Critic as Lyapunov function (CALF) Осиненко.
> Это RL-схема, где критик $$\hat q_w(s,a)$$ играет роль функции Ляпунова: $$-\hat q_w$$ ограничен сверху и снизу классом $$\mathcal K_\infty$$ и на каждом сертифицированном шаге уменьшается минимум на $$\bar\nu$$.
> В системе есть безопасная номинальная политика $$\pi_0$$, которая гарантированно приводит в целевое множество $$G$$.
> CALF строит поверх обычного актора $$\pi_t$$ «щит»:
> при каждом шаге пытается найти обновление критика, которое удовлетворяет Ляпунов-ограничениям; если это удаётся — действие актора считается безопасным.
> Если нет — с вероятностью $$1-P_{\text{relax}}$$ включается $$\pi_0$$, а с вероятностью $$P_{\text{relax}}$$ системе разрешается «пошалить» и остаться с актором.
> При этом $$P_{\text{relax}}$$ со временем экспоненциально убывает с коэффициентом $$\lambda_{\text{relax}}\in[0,1)$$, а ожидаемое число «расслаблений» конечное.
> За счёт этого сохраняется goal-reaching property номинальной политики, но в начале обучения агент получает окно для исследования и улучшения политики.

---

## 7. Что можно делать дальше в рамках проекта (без привязки к SPOR)

Это уже «чек-лист» для будущих обсуждений, не вопросы к тебе:

1. **Разобрать до конца теоретическую часть CALF**:

   * детали теорем 1/2;
   * роли $$\bar\nu, \hat\kappa_{\text{low}},\hat\kappa_{\text{up}}$$;
   * оценку времени достижения $$G$$.

2. **Сформулировать CALF в более инженерной форме**:

   * как обернуть им стандартный actor–critic / PPO / SAC;
   * какие требования к наблюдаемым величинам.

3. **Разобраться с «probabilistic mechanism for safe RL»** (работа Yaramenko/Bolychev и соавторы), где relax/probability-механизм развивается отдельно.

4. **Продумать возможную реализацию** (но код писать позже, когда ты явно скажешь, что пора).

---

Если хочешь, могу следующим шагом сделать компактную версию (1–2 абзаца) специально для системного промпта Cursor-агента, а это оставить как «толстый» справочник.
